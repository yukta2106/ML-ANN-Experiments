{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbk6/1fKdNgFeZ9Fr6ac3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukta2106/ML-ANN-Experiments/blob/main/ANN_Assignment_Unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-O4oi7kR1SA",
        "outputId": "bf411a6b-157e-428c-adee-e8be2a20e156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Diabetes dataset...\n",
            "Standardizing features...\n",
            "\n",
            "--- Training MLPRegressor (Gradient Descent) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Delta Learning Rule (Linear Model) ---\n",
            "\n",
            "==================================================\n",
            "             Model Performance Comparison\n",
            "==================================================\n",
            "**MLPRegressor (Gradient Descent)** R² Score: 0.2788\n",
            "**Delta Rule Linear Model** R² Score:     0.4527\n",
            "==================================================\n",
            "\n",
            "Observation: The performance is similar, or the Delta Rule model won. \n",
            "This would suggest that the relationship in the data is predominantly linear.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "print(\"Loading Diabetes dataset...\")\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "y_train_2d = y_train.reshape(-1, 1)\n",
        "y_test_2d = y_test.reshape(-1, 1)\n",
        "\n",
        "\n",
        "## =======================================================\n",
        "## MODEL 1: Gradient Descent (via MLPRegressor)\n",
        "## =======================================================\n",
        "\n",
        "print(\"\\n--- Training MLPRegressor (Gradient Descent) ---\")\n",
        "\n",
        "mlp = MLPRegressor(\n",
        "    hidden_layer_sizes=(50,),\n",
        "    activation='relu',\n",
        "    solver='adam',  # An optimized version of Gradient Descent\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "r2_mlp = r2_score(y_test, y_pred_mlp)\n",
        "\n",
        "\n",
        "## =======================================================\n",
        "## MODEL 2: Delta Learning Rule (Manual Implementation)\n",
        "## =======================================================\n",
        "\n",
        "print(\"--- Training Delta Learning Rule (Linear Model) ---\")\n",
        "\n",
        "class DeltaRuleLinearModel:\n",
        "    \"\"\"\n",
        "    Implements a single-layer linear model trained using the Delta Learning Rule\n",
        "    (also known as the LMS rule).\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            y_hat = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            error = y - y_hat\n",
        "\n",
        "            self.weights += self.lr * np.dot(X.T, error) / n_samples\n",
        "            self.bias += self.lr * np.sum(error) / n_samples\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "delta_model = DeltaRuleLinearModel(learning_rate=0.1, epochs=5000)\n",
        "delta_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_delta = delta_model.predict(X_test_scaled)\n",
        "\n",
        "r2_delta = r2_score(y_test, y_pred_delta)\n",
        "\n",
        "\n",
        "## =======================================================\n",
        "## COMPARISON\n",
        "## =======================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             Model Performance Comparison\")\n",
        "print(\"=\"*50)\n",
        "print(f\"**MLPRegressor (Gradient Descent)** R² Score: {r2_mlp:.4f}\")\n",
        "print(f\"**Delta Rule Linear Model** R² Score:     {r2_delta:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if r2_mlp > r2_delta:\n",
        "    print(\"\\nObservation: The MLPRegressor (full Neural Network with backpropagation) \")\n",
        "    print(\"significantly outperforms the simple Delta Rule Linear Model. \")\n",
        "    print(\"This is expected because the MLP can capture non-linear relationships \")\n",
        "    print(\"in the data, while the Delta Rule implementation here is restricted \")\n",
        "    print(\"to a simple linear function.\")\n",
        "else:\n",
        "    print(\"\\nObservation: The performance is similar, or the Delta Rule model won. \")\n",
        "    print(\"This would suggest that the relationship in the data is predominantly linear.\")"
      ]
    }
  ]
}